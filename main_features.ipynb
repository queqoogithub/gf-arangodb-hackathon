{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f3c996",
   "metadata": {},
   "source": [
    "# Job Recommendation System with Graph Database\n",
    "\n",
    "This Jupyter Notebook demonstrates how to build a job recommendation system using a graph database (ArangoDB). The process includes:\n",
    "1. Installing necessary libraries.\n",
    "2. Loading datasets and generating a directed graph using NetworkX.\n",
    "3. Storing the graph in ArangoDB.\n",
    "4. Retrieving the graph from ArangoDB for further use.\n",
    "5. Implementing a chatbot to query the graph and recommend jobs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d68718",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "We need to install the following Python libraries:\n",
    "- `pandas`: For data manipulation.\n",
    "- `networkx`: For creating and managing graphs.\n",
    "- `python-arango`: Python driver for ArangoDB.\n",
    "- `google-genai`: For interacting with Google's generative AI.\n",
    "- `fuzzywuzzy`: For fuzzy string matching.\n",
    "- `python-Levenshtein`: Speeds up string comparison operations (e.g., in fuzzywuzzy).\n",
    "- `python-dotenv`: For loading environment variables.\n",
    "- `langchain-community`: For graph-based QA chains.\n",
    "- `langchain-google-genai`: For Google AI integration with LangChain.\n",
    "- `langchain-openai`: For OpenAI integration with LangChain (optional).\n",
    "\n",
    "Run the following commands in a code cell to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fbd991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: networkx in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: python-arango in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (8.1.6)\n",
      "Collecting python-Levenshtein\n",
      "  Using cached python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: google-genai in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-dotenv in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-community in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-google-genai in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (2.0.11)\n",
      "Requirement already satisfied: langchain-openai in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (2.3.0)\n",
      "Requirement already satisfied: requests in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (2.32.3)\n",
      "Requirement already satisfied: requests_toolbelt in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (1.0.0)\n",
      "Requirement already satisfied: PyJWT in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (2.10.1)\n",
      "Requirement already satisfied: setuptools>=42 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (75.8.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (8.6.1)\n",
      "Requirement already satisfied: packaging>=23.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-arango) (24.2)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: anyio<5.0.0dev,>=4.8.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (4.8.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (2.38.0)\n",
      "Requirement already satisfied: httpx<1.0.0dev,>=0.28.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (2.11.0b1)\n",
      "Requirement already satisfied: websockets<15.0dev,>=13.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (14.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0dev,>=4.11.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-genai) (4.12.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (0.3.43)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (2.0.38)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-google-genai) (0.6.16)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-openai) (1.65.5)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from anyio<5.0.0dev,>=4.8.0->google-genai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from anyio<5.0.0dev,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from anyio<5.0.0dev,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: certifi in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0dev,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from importlib_metadata>=4.7.1->python-arango) (3.21.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.31.1 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.31.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from requests->python-arango) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0rc2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0rc2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/tanatorn/miniconda3/envs/test_arango/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Using cached python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas networkx python-arango python-Levenshtein google-genai fuzzywuzzy python-dotenv langchain-community langchain-google-genai langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6994464",
   "metadata": {},
   "source": [
    "## Step 2: Load Datasets and Generate Graph\n",
    "\n",
    "In this section, we'll load the CSV datasets and create a directed graph using NetworkX. The datasets include jobs, soft skills, hard skills, interests, and education, which will be represented as nodes and edges in the graph.\n",
    "\n",
    "We'll also define a helper function `clean_key` to sanitize keys for node identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f032ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2014\n",
      "Number of edges: 11708\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# Load CSV datasets\n",
    "df = pd.read_csv(\"dataset/sample/job.csv\")\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "def clean_key(key):\n",
    "    key_str = str(key)\n",
    "    return ''.join(c if c.isalnum() else '_' for c in key_str).strip('_').lower()\n",
    "\n",
    "# Add Jobs and attributes\n",
    "for index, row in df.iterrows():\n",
    "    job_key = \"job_\"+clean_key(row[\"job_title\"])\n",
    "    G.add_node(job_key, type=\"Job\", min_salary=row[\"min_salary\"], max_salary=row[\"max_salary\"],\n",
    "               min_exp=row[\"min_exp\"], max_exp=row[\"max_exp\"], level=row[\"level\"],\n",
    "               category=row[\"job_category\"], job_description=row[\"job_description\"],\n",
    "               name=row[\"job_title\"])\n",
    "    \n",
    "    # Soft skills\n",
    "    soft_skills = set(row[\"soft_skill\"].split(\"|\"))\n",
    "    for skill in soft_skills:\n",
    "        skill_key = \"soft_\"+clean_key(skill)\n",
    "        G.add_node(skill_key, type=\"soft_skill\", name=skill)\n",
    "        G.add_edge(skill_key, job_key, relation=\"soft_skill_leads_to\")\n",
    "\n",
    "    # Hard skills\n",
    "    hard_skills = set(row[\"hard_skill\"].split(\"|\"))\n",
    "    for skill in hard_skills:\n",
    "        skill_key = \"hard_\"+clean_key(skill)\n",
    "        G.add_node(skill_key, type=\"hard_skill\", name=skill)\n",
    "        G.add_edge(skill_key, job_key, relation=\"hard_skill_leads_to\")\n",
    "\n",
    "    # Interest\n",
    "    interests = set(row[\"interest\"].split(\"|\"))\n",
    "    for interest in interests:\n",
    "        interest_key = \"int_\"+clean_key(interest)\n",
    "        G.add_node(interest_key, type=\"interest\", name=interest)\n",
    "        G.add_edge(interest_key, job_key, relation=\"supports\")\n",
    "\n",
    "    # Education\n",
    "    educations = set(row[\"education\"].split(\"|\"))\n",
    "    for edu in educations:\n",
    "        edu_key = \"edu_\"+clean_key(edu)\n",
    "        G.add_node(edu_key, type=\"education\", name=edu)\n",
    "        G.add_edge(edu_key, job_key, relation=\"enables_to\")\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc291e",
   "metadata": {},
   "source": [
    "## Step 3: Store Graph in ArangoDB\n",
    "\n",
    "Now, we'll connect to ArangoDB, create the necessary collections, and store the graph. We'll use environment variables for credentials, so make sure you have a `.env` file with `ARANGO_HOST`, `ARANGO_PASSWORD`, etc.\n",
    "\n",
    "You can use your local ArangoDB using Docker by running this command:\n",
    "\n",
    "```bash\n",
    "docker run -d -p 8529:8529 -e ARANGO_ROOT_PASSWORD=password --name hiddenpaths_arangodb arangodb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3620d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 'job_graph' created with 4 edge definitions\n"
     ]
    }
   ],
   "source": [
    "from arango import ArangoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to ArangoDB\n",
    "client = ArangoClient(hosts=os.getenv(\"ARANGO_HOST\", \"http://localhost:8529\"))\n",
    "sys_db = client.db(\"_system\", username=\"root\", password=os.getenv(\"ARANGO_PASSWORD\", \"password\"), verify=True)\n",
    "\n",
    "# Create 'JOB' database if it doesn't exist\n",
    "if not sys_db.has_database(\"Test\"):\n",
    "    sys_db.create_database(\"Test\")\n",
    "else:\n",
    "    sys_db.delete_database(\"Test\")\n",
    "    sys_db.create_database(\"Test\")\n",
    "\n",
    "db = client.db(\"Test\", username=\"root\", password=os.getenv(\"ARANGO_PASSWORD\", \"password\"))\n",
    "\n",
    "# Define collections\n",
    "vertex_collections = [\"job\", \"soft_skill\", \"hard_skill\", \"interest\", \"education\"]\n",
    "edge_collections = [\"requires_softskill\", \"requires_hardskill\", \"supported_by_interest\", \"enables_job\"]\n",
    "\n",
    "for vc in vertex_collections:\n",
    "    if not db.has_collection(vc):\n",
    "        db.create_collection(vc)\n",
    "for ec in edge_collections:\n",
    "    if not db.has_collection(ec):\n",
    "        db.create_collection(ec, edge=True)\n",
    "\n",
    "# Map collections\n",
    "collection_mapping = {\n",
    "    \"job\": db.collection(\"job\"),\n",
    "    \"soft_skill\": db.collection(\"soft_skill\"),\n",
    "    \"hard_skill\": db.collection(\"hard_skill\"),\n",
    "    \"interest\": db.collection(\"interest\"),\n",
    "    \"education\": db.collection(\"education\")\n",
    "}\n",
    "edge_mapping = {\n",
    "    \"requires_softskill\": db.collection(\"requires_softskill\"),\n",
    "    \"requires_hardskill\": db.collection(\"requires_hardskill\"),\n",
    "    \"supported_by_interest\": db.collection(\"supported_by_interest\"),\n",
    "    \"enables_job\": db.collection(\"enables_job\")\n",
    "}\n",
    "\n",
    "# Clear existing data\n",
    "for collection in vertex_collections + edge_collections:\n",
    "    db.collection(collection).truncate()\n",
    "\n",
    "# Prepare data for insertion\n",
    "vertices_to_insert = []\n",
    "edges_to_insert = []\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    doc = data.copy()\n",
    "    doc[\"_key\"] = node\n",
    "    vertices_to_insert.append(doc)\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    node1, node2, attr = edge\n",
    "    relation = attr['relation']\n",
    "    if relation == \"soft_skill_leads_to\":\n",
    "        doc = {\"_from\": f\"soft_skill/{node1}\", \"_to\": f\"job/{node2}\", \"relation\": relation}\n",
    "    elif relation == \"hard_skill_leads_to\":\n",
    "        doc = {\"_from\": f\"hard_skill/{node1}\", \"_to\": f\"job/{node2}\", \"relation\": relation}\n",
    "    elif relation == \"supports\":\n",
    "        doc = {\"_from\": f\"interest/{node1}\", \"_to\": f\"job/{node2}\", \"relation\": relation}\n",
    "    elif relation == \"enables_to\":\n",
    "        doc = {\"_from\": f\"education/{node1}\", \"_to\": f\"job/{node2}\", \"relation\": relation}\n",
    "    edges_to_insert.append(doc)\n",
    "\n",
    "# Insert vertices\n",
    "vertices_by_collection = {vc: [] for vc in vertex_collections}\n",
    "for doc in vertices_to_insert:\n",
    "    vertices_by_collection[doc['type'].lower().replace(\" \", \"_\")].append(doc)\n",
    "\n",
    "for vc, docs in vertices_by_collection.items():\n",
    "    if docs:\n",
    "        collection_mapping[vc].insert_many(docs)\n",
    "\n",
    "# Insert edges\n",
    "edges_by_collection = {ec: [] for ec in edge_collections}\n",
    "for doc in edges_to_insert:\n",
    "    relation = doc[\"relation\"]\n",
    "    if relation == \"soft_skill_leads_to\" and doc not in edges_by_collection[\"requires_softskill\"]:\n",
    "        edges_by_collection[\"requires_softskill\"].append(doc)\n",
    "    elif relation == \"hard_skill_leads_to\" and doc not in edges_by_collection[\"requires_hardskill\"]:\n",
    "        edges_by_collection[\"requires_hardskill\"].append(doc)\n",
    "    elif relation == \"supports\" and doc not in edges_by_collection[\"supported_by_interest\"]:\n",
    "        edges_by_collection[\"supported_by_interest\"].append(doc)\n",
    "    elif relation == \"enables_to\" and doc not in edges_by_collection[\"enables_job\"]:\n",
    "        edges_by_collection[\"enables_job\"].append(doc)\n",
    "\n",
    "for ec, docs in edges_by_collection.items():\n",
    "    if docs:\n",
    "        edge_mapping[ec].insert_many(docs)\n",
    "\n",
    "# Create graph\n",
    "graph_name = \"job_graph\"\n",
    "if not db.has_graph(graph_name):\n",
    "    db.create_graph(graph_name)\n",
    "\n",
    "graph = db.graph(graph_name)\n",
    "graph.create_edge_definition(edge_collection=\"requires_softskill\", from_vertex_collections=[\"soft_skill\"], to_vertex_collections=[\"job\"])\n",
    "graph.create_edge_definition(edge_collection=\"requires_hardskill\", from_vertex_collections=[\"hard_skill\"], to_vertex_collections=[\"job\"])\n",
    "graph.create_edge_definition(edge_collection=\"supported_by_interest\", from_vertex_collections=[\"interest\"], to_vertex_collections=[\"job\"])\n",
    "graph.create_edge_definition(edge_collection=\"enables_job\", from_vertex_collections=[\"education\"], to_vertex_collections=[\"job\"])\n",
    "\n",
    "print(f\"Graph '{graph_name}' created with {len(graph.edge_definitions())} edge definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d61480",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve Graph from ArangoDB\n",
    "\n",
    "Here, we'll retrieve the graph from ArangoDB and load it back into a NetworkX DiGraph for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc245e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved graph - Number of nodes: 2014\n",
      "Retrieved graph - Number of edges: 11708\n"
     ]
    }
   ],
   "source": [
    "# Retrieve graph data from ArangoDB\n",
    "G_retrieved = nx.DiGraph()\n",
    "\n",
    "# Fetch vertices\n",
    "vertex_collections = [\"job\", \"soft_skill\", \"hard_skill\", \"interest\", \"education\"]\n",
    "for vc in vertex_collections:\n",
    "    query = f\"FOR doc IN {vc} RETURN {{_key: doc._key, data: doc}}\"\n",
    "    cursor = db.aql.execute(query)\n",
    "    for doc in cursor:\n",
    "        G_retrieved.add_node(doc[\"_key\"], **{k: v for k, v in doc[\"data\"].items() if k not in [\"_key\", \"_id\", \"_rev\"]})\n",
    "\n",
    "# Fetch edges\n",
    "edge_collections = [\"requires_softskill\", \"requires_hardskill\", \"supported_by_interest\", \"enables_job\"]\n",
    "for ec in edge_collections:\n",
    "    query = f\"FOR doc IN {ec} RETURN {{source: SPLIT(doc._from, '/')[1], target: SPLIT(doc._to, '/')[1], data: doc}}\"\n",
    "    cursor = db.aql.execute(query)\n",
    "    for doc in cursor:\n",
    "        G_retrieved.add_edge(doc[\"source\"], doc[\"target\"], **{k: v for k, v in doc[\"data\"].items() if k not in [\"_from\", \"_to\", \"_id\", \"_rev\"]})\n",
    "\n",
    "# Verify retrieved graph\n",
    "print(f\"Retrieved graph - Number of nodes: {G_retrieved.number_of_nodes()}\")\n",
    "print(f\"Retrieved graph - Number of edges: {G_retrieved.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a3d1d",
   "metadata": {},
   "source": [
    "## Before You Go Next: Get Your Google API Key\n",
    "\n",
    "To use the Google Gemini API in this notebook (e.g., for feature extraction or chatbot responses), you'll need an API Key from Google AI Studio. \n",
    "\n",
    "Don’t worry—it’s quick and easy to set up! Follow these steps to get your key in less than 5 minutes.\n",
    "\n",
    "### How to Create an API Key\n",
    "1. **Sign In with Your Google Account**  \n",
    "   - Visit [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey).  \n",
    "   - Log in with your Google account.\n",
    "\n",
    "2. **Navigate to the API Key Page**  \n",
    "   - Once in Google AI Studio, look at the top-left corner of the page. You’ll see a **\"Get API Key\"** button. Click it!\n",
    "\n",
    "3. **Generate a New API Key**  \n",
    "   - Click **\"Create API Key\"**.  \n",
    "   - You’ll be prompted to either create a new project or use an existing one. For testing, we recommend creating a new project to keep things tidy.\n",
    "\n",
    "4. **Copy Your API Key**  \n",
    "   - After creation, you’ll see your API Key (it looks like a string of letters and numbers, e.g., `AIzaSy...`).  \n",
    "   - Click the copy button to save it for later use.\n",
    "\n",
    "5. **Use It in This Notebook**  \n",
    "   - Add your API Key to a `.env` file in the same directory as this notebook,\n",
    "   - Or you can replace environment variable `\"GOOGLE_API_KEY\"` in this notebook with your API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f9644",
   "metadata": {},
   "source": [
    "## Step 5: Implement Chatbot\n",
    "\n",
    "Finally, we'll implement a chatbot that processes natural language queries, extracts features, maps them to the graph, and provides job recommendations using LangChain and ArangoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d274951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
      "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
      "WITH job, hard_skill, soft_skill, interest, education, requires_hardskill, requires_softskill, supported_by_interest, enables_job\n",
      "FOR hardSkill IN hard_skill\n",
      "  FILTER hardSkill.name IN [\"Python\", \"Java\", \"C++\"]\n",
      "  FOR job_doc IN OUTBOUND hardSkill requires_hardskill\n",
      "    FOR interest_doc IN interest\n",
      "      FILTER interest_doc.name IN [\"Machine Learning\", \"Computational Intelligence\"]\n",
      "      FOR job_doc_2 IN OUTBOUND interest_doc supported_by_interest\n",
      "        FILTER job_doc_2._id == job_doc._id\n",
      "          FOR education_doc IN education\n",
      "            FILTER education_doc.name IN [\"PhD in Computer Science\"]\n",
      "            FOR job_doc_3 IN OUTBOUND education_doc enables_job\n",
      "              FILTER job_doc_3._id == job_doc_2._id\n",
      "              RETURN DISTINCT {\n",
      "                job_name: job_doc_3.name,\n",
      "                min_salary: job_doc_3.min_salary,\n",
      "                max_salary: job_doc_3.max_salary,\n",
      "                min_exp: job_doc_3.min_exp,\n",
      "                max_exp: job_doc_3.max_exp\n",
      "              }\n",
      "\u001b[0m\n",
      "AQL Result:\n",
      "\u001b[32;1m\u001b[1;3m[{'job_name': 'Artificial Intelligence Researcher', 'min_salary': 75000, 'max_salary': 120000, 'min_exp': 4, 'max_exp': 8}, {'job_name': 'Computer Vision Researcher', 'min_salary': 75000, 'max_salary': 115000, 'min_exp': 4, 'max_exp': 8}, {'job_name': 'Neural Network Researcher', 'min_salary': 75000, 'max_salary': 115000, 'min_exp': 4, 'max_exp': 8}, {'job_name': 'Computer Vision Engineer', 'min_salary': 75000, 'max_salary': 115000, 'min_exp': 4, 'max_exp': 8}, {'job_name': 'Research Scientist', 'min_salary': 70000, 'max_salary': 110000, 'min_exp': 5, 'max_exp': 10}, {'job_name': 'Data Scientist', 'min_salary': 75000, 'max_salary': 120000, 'min_exp': 4, 'max_exp': 8}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on your background in Python, Java, C++, your interests in Machine Learning and Computational Intelligence, and your PhD in Computer Science, here are some jobs you might consider:\n",
       "\n",
       "1.  **Artificial Intelligence Researcher**: This role offers a salary range of $75,000 - $120,000 and requires 4-8 years of experience. It aligns well with your interests and educational background.\n",
       "2.  **Computer Vision Researcher**: With a salary range of $75,000 - $115,000 and requiring 4-8 years of experience, this job leverages your computer science expertise and interest in machine learning.\n",
       "3.  **Neural Network Researcher**: Similar to the Computer Vision Researcher, this position has a salary range of $75,000 - $115,000 and requires 4-8 years of experience. It also fits your skills and interest in machine learning.\n",
       "4.  **Computer Vision Engineer**: This job offers a salary range of $75,000 - $115,000 and requires 4-8 years of experience, making use of your programming skills and machine learning interests.\n",
       "5.  **Data Scientist**: This position has a salary range of $75,000 - $120,000 and requires 4-8 years of experience. It fits your background in computer science and your skills in programming."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import json\n",
    "from google import genai\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize Google AI client\n",
    "google_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Define feature extraction schema\n",
    "class Extract_feature(BaseModel):\n",
    "    job: List[str]\n",
    "    hard_skill: List[str]\n",
    "    soft_skill: List[str]\n",
    "    interest: List[str]\n",
    "    education: List[str]\n",
    "\n",
    "# Extract features from natural language query\n",
    "def extract(nlq: str, verbose=False) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI tasked with analyzing natural language input from a user and extracting specific details into a JSON format.\n",
    "    User's input: {nlq}\n",
    "    Identify and categorize the following from the user's input:\n",
    "    - 'job': job titles or roles mentioned (e.g., Software Engineer, Teacher).\n",
    "    - 'hard_skill': technical skills or specific abilities (e.g., programming languages, tools).\n",
    "    - 'soft_skill': interpersonal or cognitive skills (e.g., problem-solving, leadership).\n",
    "    - 'interest': hobbies or personal interests (e.g., gaming, reading).\n",
    "    - 'education': fields of study or degrees (e.g., Mathematics, Computer Science).\n",
    "    Return the result as a JSON object with these fields as arrays. If no information is found for a category, use an empty array [].\n",
    "    Use your judgment to interpret the input accurately, even if the phrasing varies.\n",
    "    \"\"\"\n",
    "    config = {\"response_mime_type\": \"application/json\", \"response_schema\": Extract_feature}\n",
    "    response = google_client.models.generate_content(model=\"gemini-2.0-flash\", contents=[prompt], config=config)\n",
    "    return json.loads(response.text)\n",
    "\n",
    "# Fuzzy mapping to correct input data\n",
    "def fuzzy_mapping(preprocessed_json: dict):\n",
    "    all_nodes = {}\n",
    "    for vc in [\"job\", \"soft_skill\", \"hard_skill\", \"interest\", \"education\"]:\n",
    "        query = f\"FOR doc IN {vc} RETURN doc.name\"\n",
    "        cursor = db.aql.execute(query)\n",
    "        all_nodes[vc] = [doc for doc in cursor]\n",
    "    corrected_additional_data = {}\n",
    "    for key, values in preprocessed_json.items():\n",
    "        corrected_additional_data[key] = [\n",
    "            process.extractOne(val, all_nodes[key], scorer=fuzz.token_sort_ratio)[0]\n",
    "            for val in values if process.extractOne(val, all_nodes[key], scorer=fuzz.token_sort_ratio)[1] >= 50\n",
    "        ]\n",
    "    return corrected_additional_data\n",
    "\n",
    "# Query the graph and generate response\n",
    "def text_to_aql_to_text(query: str, preprocessed_json: dict):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    arango_graph = ArangoGraph(db)\n",
    "    chain = ArangoGraphQAChain.from_llm(llm=llm, graph=arango_graph, verbose=True, allow_dangerous_requests=True)\n",
    "    combined_input = (\n",
    "            f\"Natural Language Query: {query}\\n\"\n",
    "            f\"Preprocessed Data for name attribute of node: {json.dumps(preprocessed_json)}\\n\"\n",
    "            \"Generate an AQL query based on the preprocessed data and the following database schema:\\n\"\n",
    "            \"- Nodes:\\n\"\n",
    "            \"  1. Job: {_key: startswith job_, type: Job, min_salary: int, max_salary: int, min_exp: int, max_exp: int, level: [Junior, Mid, Senior], category: str, job_description: str, name: job title}\\n\"\n",
    "            \"  2. hard_skill: {_key: startswith hard_, type: hard_skill, category: str, description: str, name: skill name}\\n\"\n",
    "            \"  3. soft_skill: {_key: startswith soft_, type: soft_skill, category: str, description: str, name: skill name}\\n\"\n",
    "            \"  4. interest: {_key: startswith int_, type: interest, category: str, name: interest name}\\n\"\n",
    "            \"  5. education: {_key: startswith edu_, type: education, category: str, name: education name}\\n\"\n",
    "            \"- Edges (direction matters):\\n\"\n",
    "            \"  1. requires_softskill: {_from: soft_skill/, _to: job/, relation: soft_skill_leads_to} (OUTBOUND from soft_skill to job)\\n\"\n",
    "            \"  2. requires_hardskill: {_from: hard_skill/, _to: job/, relation: hard_skill_leads_to} (OUTBOUND from hard_skill to job)\\n\"\n",
    "            \"  3. supported_by_interest: {_from: interest/, _to: job/, relation: supports} (OUTBOUND from interest to job)\\n\"\n",
    "            \"  4. enables_job: {_from: education/, _to: job/, relation: enables_to} (OUTBOUND from education to job)\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. Use the preprocessed JSON data (hard_skill, soft_skill, interests, education) to construct an AQL query based on the natural language query.\\n\"\n",
    "            \"2. Leverage edge relationships (e.g., requires_hardskill, enables_job) to connect the input data to relevant nodes (jobs, skills, interests, or education), paying careful attention to edge direction:\\n\"\n",
    "            \"   - Use OUTBOUND when traversing from skills, interests, or education to jobs (e.g., soft_skill -> job, hard_skill -> job).\\n\"\n",
    "            \"   - Use INBOUND when traversing from jobs to skills, interests, or education (e.g., job -> soft_skill), if applicable to the query.\\n\"\n",
    "            \"   - Respect the schema: edges are defined as OUTBOUND from skills/interests/education to jobs, so prioritize this direction unless the query explicitly requires reverse traversal.\\n\"\n",
    "            \"3. When matching jobs, allow flexibility: jobs should match at least one skill, interest, or education from the input data, not requiring all to be present.\\n\"\n",
    "            \"4. Limit the query results to a maximum of 5 objects with carefully command.\\n\"\n",
    "            \"5. Internally generate the AQL query, but do not include it in the final response.\\n\"\n",
    "            \"6. Translate the query results into a natural, user-friendly response, avoiding technical terms like 'AQL', 'INBOUND', 'OUTBOUND', 'node', or 'edge':\\n\"\n",
    "            \"   - If jobs are relevant, provide a simple list of up to 5 job titles with details like salary range (e.g., '$50,000 - $70,000') and experience required (e.g., '2-5 years'), followed by a brief explanation of why they match (e.g., 'This job fits because you know Python, which is one of the skills it needs').\\n\"\n",
    "            \"   - If skills or interests are the focus, describe up to 5 related skills or interests and how they connect to potential jobs (e.g., 'Your skill in Python could help you with software engineering roles').\\n\"\n",
    "            \"   - Use conversational language, as if explaining to a non-technical person.\\n\"\n",
    "            \"7. If no relevant results are found, respond with a simple message like 'Sorry, I couldn’t find any matches for you,' followed by a brief reason (e.g., 'None of the skills or education you provided seem to connect to the jobs available').\\n\"\n",
    "            \"Please generate the response based on the instructions and the provided data in Markdown format.\"\n",
    "        )\n",
    "    result = chain.invoke(combined_input)\n",
    "    return str(result[\"result\"])\n",
    "\n",
    "# Chatbot function\n",
    "def chatbot(query: str) -> str:\n",
    "    try:\n",
    "        extracted_json = extract(query, True)\n",
    "        corrected_json = fuzzy_mapping(extracted_json)\n",
    "        response = text_to_aql_to_text(query, corrected_json)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# For this notebook, I use dataset about technology, bussiness and creative jobs, so the chatbot will be able to answer questions related to these fields.\n",
    "query = \"I have experience in Python, Java, and C++. I am interested in Machine Learning and Artificial Intelligence. I have a degree in Computer Science. What jobs can I apply for?\"\n",
    "result = chatbot(query)\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cada3c4",
   "metadata": {},
   "source": [
    "## Step 6: Generate Career Path Diagram\n",
    "\n",
    "In this step, we’ll generate a career path diagram using the `json_for_graph` function. This feature:\n",
    "- Takes a natural language input from the user (e.g., skills, interests).\n",
    "- Extracts and matches features against the graph to find the top 3 job recommendations.\n",
    "- For each job, identifies up to 2 \"children\" jobs (higher-salary career progression options).\n",
    "- Returns a JSON structure that could be visualized as a tree-like diagram.\n",
    "\n",
    "We’ll implement the necessary functions with detailed explanations and test it with an example input. The output will be displayed as formatted Markdown for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ceee9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Define Supporting Classes and Functions ---\n",
    "\n",
    "class JobSelector:\n",
    "    \"\"\"A simple class to track selected jobs and avoid duplicates in career path generation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.selected_job = []  # List to store job keys already selected\n",
    "\n",
    "class Extract_feature(BaseModel):\n",
    "    \"\"\"Pydantic model to enforce JSON schema for extracted features from user input.\"\"\"\n",
    "    job: List[str]\n",
    "    hard_skill: List[str]\n",
    "    soft_skill: List[str]\n",
    "    interest: List[str]\n",
    "    education: List[str]\n",
    "\n",
    "def extract(nlq: str, verbose=False) -> dict:\n",
    "    \"\"\"Extract features from natural language query using Google Gemini API.\"\"\"\n",
    "    # Define the prompt to instruct the AI on feature extraction\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI tasked with analyzing natural language input from a user and extracting specific details into a JSON format.\n",
    "    User's input: {nlq}\n",
    "    Identify and categorize the following from the user's input:\n",
    "    - 'job': job titles or roles mentioned (e.g., Software Engineer, Teacher).\n",
    "    - 'hard_skill': technical skills or specific abilities (e.g., programming languages, tools).\n",
    "    - 'soft_skill': interpersonal or cognitive skills (e.g., problem-solving, leadership).\n",
    "    - 'interest': hobbies or personal interests (e.g., gaming, reading).\n",
    "    - 'education': fields of study or degrees (e.g., Mathematics, Computer Science).\n",
    "    Return the result as a JSON object with these fields as arrays. If no information is found for a category, use an empty array [].\n",
    "    Use your judgment to interpret the input accurately, even if the phrasing varies.\n",
    "    \"\"\"\n",
    "    # Configure the API to return structured JSON\n",
    "    config = {\"response_mime_type\": \"application/json\", \"response_schema\": Extract_feature}\n",
    "    # Call the Google Gemini API to generate the response\n",
    "    response = google_client.models.generate_content(model=\"gemini-2.0-flash\", contents=[prompt], config=config)\n",
    "    json_response = json.loads(response.text)  # Parse the response into a Python dict\n",
    "    if verbose:\n",
    "        print(f\"Original NLQ: {nlq}\")\n",
    "        print(\"Extracted JSON:\", json_response)\n",
    "    return json_response\n",
    "\n",
    "def fuzzy_mapping(preprocessed_json: dict, verbose=False) -> dict:\n",
    "    \"\"\"Map extracted features to existing nodes in the graph using fuzzy matching.\"\"\"\n",
    "    all_nodes = {}\n",
    "    # Fetch all node names from ArangoDB for each collection\n",
    "    for vc in [\"job\", \"soft_skill\", \"hard_skill\", \"interest\", \"education\"]:\n",
    "        query = f\"FOR doc IN {vc} RETURN doc.name\"\n",
    "        cursor = db.aql.execute(query)\n",
    "        all_nodes[vc] = [doc for doc in cursor]\n",
    "    # Correct user input by finding the closest match in the graph (threshold: 50% similarity)\n",
    "    corrected_additional_data = {}\n",
    "    for key, values in preprocessed_json.items():\n",
    "        corrected_additional_data[key] = [\n",
    "            process.extractOne(val, all_nodes[key], scorer=fuzz.token_sort_ratio)[0]\n",
    "            for val in values if process.extractOne(val, all_nodes[key], scorer=fuzz.token_sort_ratio)[1] >= 50\n",
    "        ]\n",
    "    if verbose:\n",
    "        print(\"Corrected data:\", corrected_additional_data)\n",
    "    return corrected_additional_data\n",
    "\n",
    "def required_for_job(job: str, G: nx.DiGraph) -> dict:\n",
    "    \"\"\"Get the required skills, interests, and education for a given job from the graph.\"\"\"\n",
    "    # Extract incoming edges to identify requirements\n",
    "    required_hard_skills = list(set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                     if attr['relation'] == 'hard_skill_leads_to']))\n",
    "    required_soft_skills = list(set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                     if attr['relation'] == 'soft_skill_leads_to']))\n",
    "    required_interests = list(set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                   if attr['relation'] == 'supports']))\n",
    "    required_education = list(set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                   if attr['relation'] == 'enables_to']))\n",
    "    return {\n",
    "        \"hard_skill\": required_hard_skills,\n",
    "        \"soft_skill\": required_soft_skills,\n",
    "        \"interest\": required_interests,\n",
    "        \"education\": required_education\n",
    "    }\n",
    "\n",
    "def merge_dicts(dict1: dict, dict2: dict) -> dict:\n",
    "    \"\"\"Merge two dictionaries, combining lists and removing duplicates.\"\"\"\n",
    "    merged_dict = {}\n",
    "    all_keys = set(dict1.keys()).union(set(dict2.keys()))\n",
    "    for key in all_keys:\n",
    "        merged_dict[key] = list(set(dict1.get(key, []) + dict2.get(key, [])))\n",
    "    return merged_dict\n",
    "\n",
    "def find_best_job(user_input: dict, G: nx.DiGraph) -> dict:\n",
    "    \"\"\"Find the best job matches based on user input, scoring them by skill matches.\"\"\"\n",
    "    job_nodes = [n for n, attr in G.nodes(data=True) if attr['type'] == 'Job']  # Filter job nodes\n",
    "    job_scores = []\n",
    "    mapped_node = {key: att['name'] for key, att in G.nodes(data=True)}  # Map node keys to names\n",
    "    \n",
    "    for job in job_nodes:\n",
    "        # Get job requirements\n",
    "        required_hard_skills = set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                    if attr['relation'] == 'hard_skill_leads_to'])\n",
    "        required_soft_skills = set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                    if attr['relation'] == 'soft_skill_leads_to'])\n",
    "        required_interests = set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                  if attr['relation'] == 'supports'])\n",
    "        required_education = set([n for n, t, attr in G.in_edges(job, data=True) \n",
    "                                  if attr['relation'] == 'enables_to'])\n",
    "        \n",
    "        # Map requirements to human-readable names\n",
    "        mapped_required_hard_skills = [mapped_node[hs] for hs in required_hard_skills]\n",
    "        mapped_required_soft_skills = [mapped_node[ss] for ss in required_soft_skills]\n",
    "        mapped_required_interests = [mapped_node[it] for it in required_interests]\n",
    "        mapped_required_education = [mapped_node[edu] for edu in required_education]\n",
    "        required = [mapped_required_hard_skills, mapped_required_soft_skills, mapped_required_interests, mapped_required_education]\n",
    "        \n",
    "        # Convert user input to sets for comparison\n",
    "        user_hard = set(user_input.get(\"hard_skill\", []))\n",
    "        user_soft = set(user_input.get(\"soft_skill\", []))\n",
    "        user_int = set(user_input.get(\"interest\", []))\n",
    "        user_edu = set(user_input.get(\"education\", []))\n",
    "        \n",
    "        # Calculate matches and misses\n",
    "        matched_hard = len(user_hard & set(mapped_required_hard_skills))\n",
    "        matched_soft = len(user_soft & set(mapped_required_soft_skills))\n",
    "        matched_int = len(user_int & set(mapped_required_interests))\n",
    "        matched_edu = len(user_edu & set(mapped_required_education))\n",
    "        missing_hard = len(set(mapped_required_hard_skills) - user_hard)\n",
    "        missing_soft = len(set(mapped_required_soft_skills) - user_soft)\n",
    "        \n",
    "        # Calculate score based on matches, misses, and in-degree (complexity penalty)\n",
    "        in_degree = G.in_degree(job)\n",
    "        score = (matched_hard * 10) + (matched_soft * 5) + (matched_int * 1) + (matched_edu * 0.5) - (missing_hard * 1) - (missing_soft * 0.5) - (in_degree * 0.5)\n",
    "        \n",
    "        # Compute average salary\n",
    "        avg_salary = (G.nodes[job]['min_salary'] + G.nodes[job]['max_salary']) / 2\n",
    "        job_attribute = {\n",
    "            \"job_title\": G.nodes[job]['name'],\n",
    "            \"category\": G.nodes[job]['category'],\n",
    "            \"job_description\": G.nodes[job]['job_description'],\n",
    "            \"min_salary\": G.nodes[job]['min_salary'],\n",
    "            \"max_salary\": G.nodes[job]['max_salary'],\n",
    "            \"min_exp\": G.nodes[job]['min_exp'],\n",
    "            \"max_exp\": G.nodes[job]['max_exp'],\n",
    "            \"level\": G.nodes[job]['level'],\n",
    "            \"average_salary\": avg_salary\n",
    "        }\n",
    "        \n",
    "        # Store job details and score\n",
    "        job_scores.append((job, score, matched_hard, matched_soft, missing_hard, missing_soft, in_degree, job_attribute, required))\n",
    "    \n",
    "    # Sort jobs by score (highest first)\n",
    "    sorted_jobs = sorted(job_scores, key=lambda x: x[1], reverse=True)\n",
    "    result = {\n",
    "        'all_scores': [\n",
    "            {\n",
    "                'job_key': job,\n",
    "                'score': score,\n",
    "                'matched_hard': m_hard,\n",
    "                'matched_soft': m_soft,\n",
    "                'missing_hard': miss_hard,\n",
    "                'missing_soft': miss_soft,\n",
    "                'in_degree': in_deg,\n",
    "                'attribute': attribute,\n",
    "                'hard_skill': required[0],\n",
    "                'soft_skill': required[1],\n",
    "                'interest': required[2],\n",
    "                'education': required[3]\n",
    "            } for job, score, m_hard, m_soft, miss_hard, miss_soft, in_deg, attribute, required in sorted_jobs\n",
    "        ]\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def find_children(job: str, avg_salary: float, G: nx.DiGraph, user_correct_json: dict, job_selector: JobSelector) -> list:\n",
    "    \"\"\"Find up to 2 higher-salary job progression options (children) for a given job.\"\"\"\n",
    "    required = required_for_job(job, G)  # Get requirements for the current job\n",
    "    merge_required = merge_dicts(required, user_correct_json)  # Combine with user input\n",
    "    results = find_best_job(merge_required, G)  # Find potential next jobs\n",
    "    children = []\n",
    "    for result in results['all_scores']:\n",
    "        if len(children) >= 2:  # Limit to 2 children\n",
    "            break\n",
    "        # Check if job hasn’t been selected and offers higher salary\n",
    "        if result['job_key'] not in job_selector.selected_job and result[\"attribute\"][\"average_salary\"] > avg_salary:\n",
    "            job_selector.selected_job.append(result['job_key'])\n",
    "            children.append({\n",
    "                \"job\": result[\"attribute\"][\"job_title\"],\n",
    "                \"min_salary\": result[\"attribute\"][\"min_salary\"],\n",
    "                \"max_salary\": result[\"attribute\"][\"max_salary\"],\n",
    "                \"min_exp\": result[\"attribute\"][\"min_exp\"],\n",
    "                \"max_exp\": result[\"attribute\"][\"max_exp\"],\n",
    "                \"level\": result[\"attribute\"][\"level\"],\n",
    "                \"category\": result[\"attribute\"][\"category\"],\n",
    "                \"job_description\": result[\"attribute\"][\"job_description\"],\n",
    "                \"hard_skill\": result[\"hard_skill\"],\n",
    "                \"soft_skill\": result[\"soft_skill\"],\n",
    "                \"interest\": result[\"interest\"],\n",
    "                \"education\": result[\"education\"]\n",
    "            })\n",
    "    return children\n",
    "\n",
    "def json_for_graph(user_input: str) -> dict:\n",
    "    \"\"\"Generate a JSON structure representing a career path diagram.\"\"\"\n",
    "    job_selector = JobSelector()  # Initialize job selector to track used jobs\n",
    "    extracted_json = extract(user_input)  # Extract features from user input\n",
    "    corrected_json = fuzzy_mapping(extracted_json)  # Correct features using fuzzy matching\n",
    "    results = find_best_job(corrected_json, G)  # Find top job matches\n",
    "    # Mark top 3 jobs as selected\n",
    "    for j in results['all_scores'][:3]:\n",
    "        job_selector.selected_job.append(j['job_key'])\n",
    "    # Build JSON response with top 3 jobs and their children\n",
    "    json_response = {\n",
    "        \"nlq\": user_input,\n",
    "        \"user_input\": corrected_json,\n",
    "        \"nodes\": [{\n",
    "            \"job\": r[\"attribute\"][\"job_title\"],\n",
    "            \"min_salary\": r[\"attribute\"][\"min_salary\"],\n",
    "            \"max_salary\": r[\"attribute\"][\"max_salary\"],\n",
    "            \"min_exp\": r[\"attribute\"][\"min_exp\"],\n",
    "            \"max_exp\": r[\"attribute\"][\"max_exp\"],\n",
    "            \"level\": r[\"attribute\"][\"level\"],\n",
    "            \"category\": r[\"attribute\"][\"category\"],\n",
    "            \"job_description\": r[\"attribute\"][\"job_description\"],\n",
    "            \"hard_skill\": r[\"hard_skill\"],\n",
    "            \"soft_skill\": r[\"soft_skill\"],\n",
    "            \"interest\": r[\"interest\"],\n",
    "            \"education\": r[\"education\"],\n",
    "            \"children\": find_children(r[\"job_key\"], r[\"attribute\"][\"average_salary\"], G, corrected_json, job_selector)\n",
    "        } for r in results['all_scores'][:3]]\n",
    "    }\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd50529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Career Path Recommendations\n",
       "\n",
       "**Your Input:** I have experience in sustainability plan, environmental management, and also have creativity. I am interested in gardening and sustainability. I have a degree in Agriculture.\n",
       "\n",
       "### Top Job Recommendations\n",
       "\n",
       "#### 1. Organic Farmer\n",
       "- **Salary Range:** $40000 - $65000\n",
       "- **Experience Required:** 2 - 10 years\n",
       "- **Level:** Journey\n",
       "- **Category:** Agriculture\n",
       "- **Description:** Grows crops and raises livestock using organic methods.\n",
       "- **Hard Skills:** Environmental Management, Sustainability Planning, Quality Control, Operations Management, Process Improvement\n",
       "- **Soft Skills:** Persistence, Problem Solving, Communication, Creativity, Sustainability Awareness\n",
       "- **Interests:** Gardening, Healthy Living, Sustainability, Composting\n",
       "- **Education:** Bachelor's in Sustainability, Bachelor's in Agriculture\n",
       "\n",
       "**Potential Career Progression:**\n",
       "\n",
       "- **1. Soil Conservationist**\n",
       "  - **Salary Range:** $50000 - $75000\n",
       "  - **Experience Required:** 2 - 8 years\n",
       "  - **Level:** Journey\n",
       "  - **Category:** Environment\n",
       "  - **Description:** Implements strategies to prevent soil erosion and degradation.\n",
       "\n",
       "- **2. Sustainability Manager**\n",
       "  - **Salary Range:** $65000 - $95000\n",
       "  - **Experience Required:** 4 - 8 years\n",
       "  - **Level:** Senior\n",
       "  - **Category:** Business\n",
       "  - **Description:** Transportation\n",
       "\n",
       "#### 2. Sustainability Coordinator\n",
       "- **Salary Range:** $50000 - $80000\n",
       "- **Experience Required:** 2 - 8 years\n",
       "- **Level:** Journey\n",
       "- **Category:** Environment\n",
       "- **Description:** Develops programs to promote sustainable agricultural practices.\n",
       "- **Hard Skills:** Environmental Management, Sustainability Planning, Project Management, Data Analysis, Strategic Planning\n",
       "- **Soft Skills:** Problem Solving, Communication, Creativity, Sustainability Awareness, Stakeholder Management\n",
       "- **Interests:** Research, Sustainability, Environmental Management, Green Initiatives\n",
       "- **Education:** Bachelor's in Sustainability, Master's in Sustainability\n",
       "\n",
       "**Potential Career Progression:**\n",
       "\n",
       "- **1. Sustainability Consultant**\n",
       "  - **Salary Range:** $55000 - $85000\n",
       "  - **Experience Required:** 3 - 7 years\n",
       "  - **Level:** Mid\n",
       "  - **Category:** Business\n",
       "  - **Description:** Transportation\n",
       "\n",
       "- **2. Chief Sustainability Officer**\n",
       "  - **Salary Range:** $80000 - $130000\n",
       "  - **Experience Required:** 5 - 10 years\n",
       "  - **Level:** Senior\n",
       "  - **Category:** Business\n",
       "  - **Description:** Transportation\n",
       "\n",
       "#### 3. Environmental Designer\n",
       "- **Salary Range:** $50000 - $80000\n",
       "- **Experience Required:** 3 - 7 years\n",
       "- **Level:** Mid\n",
       "- **Category:** Creative\n",
       "- **Description:** Transportation\n",
       "- **Hard Skills:** Resource Allocation, Environmental Management, Sustainability Planning, Construction Management, Architecture, Solidity\n",
       "- **Soft Skills:** Problem Solving, Technical Understanding, Communication, Research, Creativity, Sustainability Awareness, Spatial Thinking\n",
       "- **Interests:** Business Planning, Ecology, Sustainable Design, Green Business\n",
       "- **Education:** Master's in Environmental Science, Bachelor's in Architecture, Master's in Sustainability\n",
       "\n",
       "**Potential Career Progression:**\n",
       "\n",
       "- **1. Environmental Engineer**\n",
       "  - **Salary Range:** $65000 - $105000\n",
       "  - **Experience Required:** 3 - 10 years\n",
       "  - **Level:** Journey\n",
       "  - **Category:** Environment\n",
       "  - **Description:** Designs systems to mitigate environmental damage and pollution.\n",
       "\n",
       "- **2. Environmental Scientist**\n",
       "  - **Salary Range:** $60000 - $90000\n",
       "  - **Experience Required:** 2 - 8 years\n",
       "  - **Level:** Journey\n",
       "  - **Category:** Environment\n",
       "  - **Description:** Studies environmental impacts and develops solutions for pollution control.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Career path JSON saved to 'career_path.json'\n"
     ]
    }
   ],
   "source": [
    "# --- Test the Career Path Generation ---\n",
    "\n",
    "# Example user input\n",
    "user_input = \"I have experience in sustainability plan, environmental management, and also have creativity. I am interested in gardening and sustainability. I have a degree in Agriculture.\"\n",
    "\n",
    "# Generate career path JSON\n",
    "career_path = json_for_graph(user_input)\n",
    "\n",
    "# Format the output as Markdown for readability\n",
    "markdown_output = f\"\"\"\n",
    "## Career Path Recommendations\n",
    "\n",
    "**Your Input:** {career_path['nlq']}\n",
    "\n",
    "### Top Job Recommendations\n",
    "\"\"\"\n",
    "for i, node in enumerate(career_path['nodes'], 1):\n",
    "    markdown_output += f\"\"\"\n",
    "#### {i}. {node['job']}\n",
    "- **Salary Range:** ${node['min_salary']} - ${node['max_salary']}\n",
    "- **Experience Required:** {node['min_exp']} - {node['max_exp']} years\n",
    "- **Level:** {node['level']}\n",
    "- **Category:** {node['category']}\n",
    "- **Description:** {node['job_description']}\n",
    "- **Hard Skills:** {', '.join(node['hard_skill']) or 'None'}\n",
    "- **Soft Skills:** {', '.join(node['soft_skill']) or 'None'}\n",
    "- **Interests:** {', '.join(node['interest']) or 'None'}\n",
    "- **Education:** {', '.join(node['education']) or 'None'}\n",
    "\n",
    "**Potential Career Progression:**\n",
    "\"\"\"\n",
    "    if node['children']:\n",
    "        for j, child in enumerate(node['children'], 1):\n",
    "            markdown_output += f\"\"\"\n",
    "- **{j}. {child['job']}**\n",
    "  - **Salary Range:** ${child['min_salary']} - ${child['max_salary']}\n",
    "  - **Experience Required:** {child['min_exp']} - {child['max_exp']} years\n",
    "  - **Level:** {child['level']}\n",
    "  - **Category:** {child['category']}\n",
    "  - **Description:** {child['job_description']}\n",
    "\"\"\"\n",
    "    else:\n",
    "        markdown_output += \"- No higher-salary progression options found.\\n\"\n",
    "\n",
    "# Display the formatted output\n",
    "display(Markdown(markdown_output))\n",
    "\n",
    "# Optionally, save the JSON to a file\n",
    "with open(\"career_path.json\", \"w\") as f:\n",
    "    json.dump(career_path, f, indent=4)\n",
    "print(\"Career path JSON saved to 'career_path.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bbd2d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully demonstrates a comprehensive job recommendation system using a graph database (ArangoDB) and AI-powered features:\n",
    "- Generated a graph from CSV datasets using NetworkX.\n",
    "- Stored the graph in ArangoDB for persistent storage.\n",
    "- Retrieved the graph for validation and further processing.\n",
    "- Implemented a chatbot that queries the graph and provides conversational job recommendations.\n",
    "- Added a career path generation feature that recommends top jobs and their potential progression paths, output as a JSON structure suitable for visualization.\n",
    "\n",
    "With these capabilities, you can explore job matches based on skills and interests, get natural language responses, and plan career growth with a structured progression diagram. To extend this further, consider visualizing the career path as a tree diagram (e.g., using `graphviz`) or enhancing the scoring algorithm for more personalized recommendations. Happy career planning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_arango",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
